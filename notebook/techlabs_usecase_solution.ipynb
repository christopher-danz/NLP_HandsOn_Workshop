{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:57:51.379995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 14:57:51.380075: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.getcwd().split(os.sep)[-1] == 'notebook':\n",
    "    os.chdir('..')\n",
    "\n",
    "from data_preprocessing import sample_class_data\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.language import Language\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_suffix_regex\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./data/\"\n",
    "PREPROCESSED_DATA_PATH = DATA_FOLDER + 'preprocessed_data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical application - Predicting the rating of a movie review\n",
    "To be able to make a prediction about the rating of a movie review (from bad/0 to very good/5), this notebook will guide you through the steps needed for this use case. To get a better understanding about the different parts of a NLP project, three parts will be looked at in detail: \n",
    "1. Data preprocessing\n",
    "1. Creating datasets for machine learninig tasks\n",
    "1. Training a model and making predictions\n",
    "\n",
    "\n",
    "For each part, there is at least one task with two possible approaches depending on your knowledge about NLP and machine learning in general. \n",
    "\n",
    "Besides this notebook, there are other additional files and folders that will be used throughout the use case. You do not have to worry about additional imports, everything is already set up for you. The files and folders are:\n",
    "* data/: folder containing the data that will be used. Please do not make any changes to the content of this folder!\n",
    "* data_preprocessing.py: Contains the function 'sample_class_data' which will be used for the part 'Training a model and making predictions'. This function balances the distribution of classes in a dataset and can apply oversampling and undersampling\n",
    "\n",
    "Before you start your work, please make sure that everything is set up correctly (see README.md)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The goal of this part is to show the steps needed to preprocess data for NLP tasks or machine learning tasks in general. These include\n",
    "* reading the data\n",
    "* filtering out unsusable data like empty lines or duplicates\n",
    "* tokenising, and depending on the use case\n",
    "    * lemmatising\n",
    "    * stemming\n",
    "    * Part of Speech (POS) detection\n",
    "    * Named Entity Recognition (NER)\n",
    "* processing classes for predictions (simplyfing, casting to other types, ...)\n",
    "* saving the processed data into an easy to use format\n",
    "\n",
    "Important:\n",
    "Instead of applying your code on all the data (data/filmstarts.tsv), please use the file 'selection_film_reviews.tsv' instead. This contains a small collection of the original dataset with which you can run this part of the use case (for the other parts of the use, an already processed dataset is available). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for the tasks below - loading a spaCy model with a custom tokenizer component, loading the Snowball Stemmer for stemming, \n",
    "# and loading LanguageIdentifier for determining the language of a text\n",
    "de_sm = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def custom_tokenizer(model):\n",
    "    infix_re = re.compile(r'''[.\\,\\_\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(model.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(model.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(model.vocab, prefix_search=prefix_re.search,\n",
    "                                  suffix_search=suffix_re.search,\n",
    "                                  infix_finditer=infix_re.finditer,\n",
    "                                  token_match=None)\n",
    "\n",
    "de_sm.tokenizer = custom_tokenizer(de_sm)\n",
    "\n",
    "snow_stemmer_de = SnowballStemmer('german')\n",
    "\n",
    "langid_model = LanguageIdentifier.from_modelstring(model, norm_probs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from a tsv/csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Write a function that loads the content of a .tsv or .csv file (consider the encoding of the file)\n",
    "def read_csv(path: str, encoding: str = \"utf-8\", newline: str = \"\\r\\n\",\n",
    "            delimiter:str = \";\") -> List:\n",
    "    \"\"\"Parses the given CSV and returns an array which contains each row of the\n",
    "    CSV.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV which should be read\n",
    "        encoding (str, optional): The CSV encoding. Defaults to \"utf-8\".\n",
    "        newline (str, optional): The newline delimiter. Defaults to \"\\r\\n\".\n",
    "        delimiter (str, optional): The delimiter between each Entry.\n",
    "            Defaults to \";\".\n",
    "\n",
    "    Returns:\n",
    "        List: Containing the parsed entries from each row.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_array = []\n",
    "    \n",
    "    # start of task\n",
    "    # TODO open the file\n",
    "    with open(path, encoding=encoding, newline=newline) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        # TODO read content of file\n",
    "        for row in csv_reader:\n",
    "            csv_array.append(row)\n",
    "    # end of task\n",
    "    return csv_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regex to find and group together repetitions of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Write a function that uses regular expressions to find repetitions of a word in a sentence and return said word. If no repetitions have been found, return the input text.\n",
    "\n",
    "# For the easier version, use the regular expressions from easy_code_help.txt and try to understand them\n",
    "# Complex version: write the regular expressions yourself\n",
    "\n",
    "# TODO write regex to find repetitions of a word at the start of a sentence\n",
    "REPEATER_fast = r\"^(.+?).{0,2}\\1+?$\"\n",
    "# TODO write regex to group all repetitions of a word\n",
    "REPEATER_exact = r\"^(.+?)(.{0,2}\\1)+?$\"\n",
    "\n",
    "\n",
    "def remove_repetitions(s):        \n",
    "    # start easy task\n",
    "    # TODO check if input text contains repetitions\n",
    "    match_fast = re.search(REPEATER_fast, s)\n",
    "    if match_fast:\n",
    "        # TODO apply the regex for grouping together reptitions and return repeated word\n",
    "        match_exact = re.search(REPEATER_exact, s)\n",
    "        return match_exact.group(1)\n",
    "    return s\n",
    "    # end of task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, lemmatization, and stemming of movie reviews using spaCy (https://spacy.io/usage/spacy-101#annotations) and nltk's SnowballStemmer (https://www.nltk.org/api/nltk.stem.snowball.html#module-nltk.stem.snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Write a function that tokenizes, lemmatizes, and applies stemming on an input text using a spaCy model and the Snowball Stemmer. \n",
    "def tokenize_and_transform(text: str,\n",
    "                           spacy_model: Language=de_sm,\n",
    "                           stemming: bool=False,\n",
    "                           lemmatization: bool=False,\n",
    "                           stopword_removal: bool=False,\n",
    "                           punctuation_removal: bool=False) -> List[str]:\n",
    "    \"\"\"Tokenizes and transforms the input text with the use of a spacy model\n",
    "    and the nltk snowball stemmer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text which should be tokenized and transformed.\n",
    "        spacy_model (Language): The Spacy model which tokenizes and lemmazes\n",
    "            the text.  Defaults to de_sm.\n",
    "        stemming (bool, optional): Tokens are stemmed if set to True. Defaults \n",
    "            to False.\n",
    "        lemmatization (bool, optional): Tokens are lemmatized if set to True.\n",
    "            Defaults to False.\n",
    "        stopword_removal (bool, optional): Stopwrod tokens are removed if set \n",
    "            to True. Defaults to False.\n",
    "        punctuation_removal (bool, optional): Punctuation tokens are removed if\n",
    "            set to True. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Transformed tokens of the input text\n",
    "    \"\"\"\n",
    "    transformed_text = []\n",
    "    # defining components of the spaCy model\n",
    "    docs = spacy_model.pipe([text], disable=[\"tagger\", \n",
    "                                             \"parser\", \n",
    "                                             \"attribute_ruler\", \n",
    "                                             \"ner\"])\n",
    "    # start of task\n",
    "    # TODO Iterate over all texts and tokens\n",
    "    for doc in docs:\n",
    "        for t in doc:\n",
    "            # TODO Check if stopword removal should be applied and the current token is a stopword and check if puncuation removal should be applied and if the current token is a punctuation\n",
    "            if (not (stopword_removal and t.is_stop) and\n",
    "                not (punctuation_removal and t.is_punct)):\n",
    "                token = t\n",
    "                # TODO apply lemmatization if requested \n",
    "                if lemmatization:\n",
    "                    token = token.lemma_\n",
    "                # TODO apply stemming if requested - use snow_stemmer_de as spaCy does not support stemming\n",
    "                if stemming:\n",
    "                    token = snow_stemmer_de.stem(str(token))\n",
    "                # TODO add token to output if it is not empty and is not '--' or ' '\n",
    "                if len(str(token)) > 1 and str(token) != '--' and ' ' not in str(token):\n",
    "                    transformed_text.append(str(token))\n",
    "    # end of task\n",
    "    return transformed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function of the preprocessing task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Write a function that iterates over the data and applies the following\n",
    "# - read each line and add them to a list, skip empty lines\n",
    "# - determine the language of a line and skip non-German lines\n",
    "# - Check for word repetitions and group repetitions\n",
    "# - tokenize and transform each line\n",
    "# - transform the value of the rating to a data type more usable than String\n",
    "def preprocess_movie_data(movie_data: List, enable_langid: bool=False) -> List:\n",
    "    \"\"\"Standardizes and preprocesses the unprocessed movie data.\n",
    "    \n",
    "    Args:\n",
    "        movie_data (List): Unprocessed movie data\n",
    "        enable_langid (Bool, optional): Enables the language detection which\n",
    "            removes foreign languages and unidentifiable texts. This slows down\n",
    "            the performance significantly. Defaults to False\n",
    "    Returns:\n",
    "        List: Preprocessed movie data\n",
    "    \"\"\"    \n",
    "    preprocessed_data = []\n",
    "    for row in tqdm(movie_data):\n",
    "        # start of task\n",
    "        # TODO skip lines with missing content\n",
    "        if len(row) < 3:\n",
    "            continue       \n",
    "        # TODO skip lines with empty values\n",
    "        if row[0] in [' ', ''] or row[1] in [' ', ''] or row[2] in [' ', '']:\n",
    "            continue\n",
    "        # TODO group together the review text if it is contained in more than one column \n",
    "        if len(row) > 3:\n",
    "            row = [row[0], row[1], ' '.join(row[2:])]\n",
    "\n",
    "        # TODO determine the language of the current line and skip non-German ones using langid_model if requested\n",
    "        if enable_langid:\n",
    "            pred_lang = langid_model.classify(row[2]) \n",
    "            if  pred_lang[0] != 'de':\n",
    "                continue\n",
    "        \n",
    "        # TODO Check for word repetitions and group repetitions \n",
    "        movie_review = remove_repetitions(row[2]).replace(\"\\n\", \" \")\n",
    "        # TODO tokenize and transform current line\n",
    "        tokens = tokenize_and_transform(movie_review,\n",
    "                                        stemming=True,\n",
    "                                        lemmatization=True,\n",
    "                                        stopword_removal=True,\n",
    "                                        punctuation_removal=True)\n",
    "        # TODO transform the review rating into a good data format and append formatted line to output as [URL, rating, raw text, transformed text]\n",
    "        preprocessed_data.append([row[0], int(row[1]), movie_review, tokens])\n",
    "    # end of task\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Write a function that uses all the functions you worked on above to save the preprocessed data as a .json\n",
    "def create_preprocessed_dataset(path_to_tsv):\n",
    "    # TODO read data from .tsv file\n",
    "    csv_array = read_csv(path_to_tsv,\n",
    "                         encoding=\"utf-8\",\n",
    "                         newline=\"\\n\",\n",
    "                         delimiter=\"\\t\")\n",
    "\n",
    "    # TODO preprocess data\n",
    "    movie_data = preprocess_movie_data(csv_array, enable_langid=True)\n",
    "\n",
    "    # TODO zip data and save in a data format usable for JSON exports\n",
    "    url, stars, text, tokens = zip(*movie_data)\n",
    "\n",
    "    preprocessed_data = {\n",
    "        'url': list(url),\n",
    "        'text_transformed': tokens,\n",
    "        'text': list(text),\n",
    "        'stars': list(stars)\n",
    "    }\n",
    "    \n",
    "    # TODO Save the data to the file 'data/preprocessed_notebook.json'\n",
    "    with open(DATA_FOLDER + 'preprocessed_notebook.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(preprocessed_data, f, ensure_ascii=False)\n",
    "\n",
    "    # End of task\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 89.66it/s]\n"
     ]
    }
   ],
   "source": [
    "exercise_block_1_output = create_preprocessed_dataset(DATA_FOLDER + 'selection_film_reviews.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets for machine learninig tasks\n",
    "As for all machine learning tasks, creating datasets for training and evaluation your model is needed. The goal of this part is to create a training, validation, and test dataset with regards to correct formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for tasks below\n",
    "\n",
    "# Loading a spaCy model\n",
    "de_core = spacy.load(\"de_core_news_lg\", disable=[\"tagger\",\n",
    "                                                 \"morphologizer\",\n",
    "                                                 \"parser\",\n",
    "                                                 \"lemmatizer\",\n",
    "                                                 \"attribute_ruler\",\n",
    "                                                 \"ner\"])\n",
    "\n",
    "# loading preprocessed data and their vector representations\n",
    "data_de_core_vectors = np.load(DATA_FOLDER + \"preprocessed_de_core_vecs.npz\")\n",
    "preprocessed_de_core_vectors = dict(data_de_core_vectors)\n",
    "\n",
    "with open(PREPROCESSED_DATA_PATH, 'r') as f:\n",
    "    preprocessed_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Depending on what you choose, set use_easy_code to True for easy version, False for complex version\n",
    "use_easy_code = False\n",
    "\n",
    "# Task: Write a function that creates lists containing different type of information, see comments for more detail\n",
    "def format_dataset(processed_data, processed_data_vectors):\n",
    "    x = [] # contains preprocessed texts\n",
    "    x_fulltext = [] # contains raw texts\n",
    "    x_vectors = [] # contains vector representations of texts\n",
    "    y = [] # contains movie ratings\n",
    "\n",
    "    # Start of task\n",
    "    # TODO iterate over the data and append the information of each line to the correct list from above\n",
    "    for i in range (len(processed_data['text_transformed'])):\n",
    "        x.append(processed_data['text_transformed'][i])\n",
    "        x_vectors.append(processed_data_vectors['vectors'][i])\n",
    "        x_fulltext.append(processed_data['text'][i])\n",
    "        y.append(processed_data['stars'][i])\n",
    "\n",
    "    # End of task\n",
    "    return x, x_fulltext, x_vectors, y\n",
    "\n",
    "# Task: Create a training, validation, and test dataset with a distribution of 70:15:15 (70% of the data in the training set, and 15% each of the data in the validation and test set)\n",
    "\n",
    "# Easy version: Use the Python library 'sklearn' to create these sets \n",
    "def split_dataset_sklearn_version(x, y, random_seed=42):\n",
    "    split_data = {}\n",
    "    # TODO use sklearn to create the train and validation-test set (distribution: 70%-30%)\n",
    "    x_train, x_val_test, y_train, y_val_test = train_test_split(x,\n",
    "                                                                y,\n",
    "                                                                test_size=0.30,\n",
    "                                                                random_state=random_seed)\n",
    "    # TODO with x_val_test and y_val_test, use sklearn to create the validation and test set (distribution: 50%-50%)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_val_test,\n",
    "                                                    y_val_test,\n",
    "                                                    test_size=0.50,\n",
    "                                                    random_state=random_seed)\n",
    "    \n",
    "    \n",
    "    split_data = {\n",
    "        'x_train': x_train,\n",
    "        'x_val': x_val,\n",
    "        'x_test': x_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    return split_data \n",
    "# End of easy version\n",
    "\n",
    "# Complex version: Write a function that creates these datasets without the help of sklearn or any other library that can create suchs sets. \n",
    "# Keep in mind that shuffling the data is important!\n",
    "def split_dataset_code_version(x, y, percent_val, percent_test, random_seed=42):\n",
    "    split_data = {}\n",
    "    \n",
    "    # TODO Check if x and y have the same length and return a ValueError of not\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError('X and Y have different lenghts.')\n",
    "\n",
    "    # TODO generate random seed and shuffle data\n",
    "    random.seed(random_seed)\n",
    "    x_y = list(zip(x, y))\n",
    "    random.shuffle(x_y)\n",
    "    x, y = zip(*x_y)\n",
    "    \n",
    "    # TODO determine the percentage value of the size of the training data \n",
    "    percent_train = 1 - percent_val - percent_test\n",
    "    \n",
    "    # TODO define indices to access the correct amount of data for training and validation\n",
    "    train_pos = int(len(x) * percent_train)\n",
    "    val_pos = int(len(x) * (percent_train + percent_val))\n",
    "    \n",
    "    split_data = {\n",
    "        'x_train': x[:train_pos],\n",
    "        'x_val': x[train_pos:val_pos],\n",
    "        'x_test': x[val_pos:],\n",
    "        'y_train': y[:train_pos],\n",
    "        'y_val': y[train_pos:val_pos],\n",
    "        'y_test': y[val_pos:]\n",
    "    }\n",
    "    # End of task\n",
    "    return split_data\n",
    "\n",
    "\n",
    "x, x_fulltext, x_vectors, y = format_dataset(preprocessed_data, preprocessed_de_core_vectors)\n",
    "\n",
    "if use_easy_code:\n",
    "    split_data = split_dataset_sklearn_version(preprocessed_data['text_transformed'],\n",
    "                            preprocessed_data['stars'])\n",
    "    task_1_split = split_dataset_sklearn_version(x, y)\n",
    "    task_1_fulltext_split = split_dataset_sklearn_version(x_fulltext, y)\n",
    "    task_1_vectors_split = split_dataset_sklearn_version(x_vectors, y)\n",
    "else:\n",
    "    split_data = split_dataset_code_version(preprocessed_data['text_transformed'],\n",
    "                            preprocessed_data['stars'],\n",
    "                            0.15,\n",
    "                            0.15)\n",
    "    task_1_split = split_dataset_code_version(x, y, 0.15, 0.15)\n",
    "    task_1_fulltext_split = split_dataset_code_version(x_fulltext, y, 0.15, 0.15)\n",
    "    task_1_vectors_split = split_dataset_code_version(x_vectors, y, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the representation of the data using One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels_task_1 = np.max(task_1_split['y_train']) + 1\n",
    "\n",
    "# functions to apply one hot encoding \n",
    "def num_to_one_hot(num, length):\n",
    "    one_hot = [0]*length\n",
    "    one_hot[num] = 1\n",
    "    return one_hot\n",
    "\n",
    "def y_to_one_hot(y, length):\n",
    "    return np.array([np.array(num_to_one_hot(e, length)) if isinstance(e, int) else e for e in y])\n",
    "\n",
    "\n",
    "# Task: Write a function that transforms the data into numpy arrays. \n",
    "# For each type of data set, data_split should contain a numpy representation of the text data (x sets) and a one hot encoding representation of the ratings (y sets).\n",
    "def keras_transform_data(data_split):\n",
    "    # Start of task\n",
    "\n",
    "    # TODO create a numpy array for the number of labels\n",
    "    num_labels = np.max(data_split['y_train']) + 1\n",
    "\n",
    "    # TODO create numpy arrays for x_train and y_train\n",
    "    data_split['x_train'] = np.array([np.array(e) if not isinstance(e, np.ndarray) else e for e in data_split['x_train']])\n",
    "    data_split['y_train'] = y_to_one_hot(data_split['y_train'], num_labels)\n",
    "    \n",
    "    # TODO create numpy arrays for x_val and y_val\n",
    "    data_split['x_val'] = np.array([np.array(e) if not isinstance(e, np.ndarray) else e for e in data_split['x_val']])\n",
    "    data_split['y_val'] = y_to_one_hot(data_split['y_val'], num_labels)\n",
    "    \n",
    "    # TODO create numpy arrays for x_text and y_test\n",
    "    data_split['x_test'] = np.array([np.array(e) if not isinstance(e, np.ndarray) else e for e in data_split['x_test']])\n",
    "    data_split['y_test'] = y_to_one_hot(data_split['y_test'], num_labels)\n",
    "    # End of task\n",
    "    return data_split\n",
    "\n",
    "keras_task_1_split = keras_transform_data(task_1_vectors_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model and making predictions\n",
    "The goal of this part is to create a machine learning model that predicts the rating of a movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training the model and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Using keras, write a function that creates the machine learning model.\n",
    "\n",
    "# For the easier version, take the code from easy_code_help.txt\n",
    "def get_model(n_inputs, n_outputs) -> Sequential:\n",
    "    model = Sequential()\n",
    "    # TODO create the structure of the Model with three Dense layer and the correct loss function and optimizer\n",
    "    # TODO add first Dense layer\n",
    "    model.add(Dense(1000, input_dim=n_inputs, activation='relu'))\n",
    "    # TODO add second Dense Layer \n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    # TODO add third Dense Layer\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    # TODO apply correct loss function and optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Task: Write a function that creates the machine learning model and applies a training and evaluation phase\n",
    "def train_keras_model(data, batch_size=16, sampling_method=\"none\", epochs=1):\n",
    "    # Start of task\n",
    "    model = None\n",
    "    # TODO define the input and output dimensions of the model and initialize it with this information\n",
    "    n_inputs, n_outputs = len(data['x_train'][0]), len(data['y_train'][0])\n",
    "    model = get_model(n_inputs, n_outputs)\n",
    "    \n",
    "    # TODO Using the sample_class_data function from data_preprocessing.py, create x_train and y_train with a sampling method\n",
    "    x_train, y_train = sample_class_data(data['x_train'],\n",
    "                                         data['y_train'],\n",
    "                                         method=sampling_method)\n",
    "    # TODO convert x_train and y_train to numpy arrays\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # TODO train the model\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              epochs=epochs)\n",
    "    \n",
    "    # TODO evaluate the model on the test data set and print out the f1 score of said evaluation (see import function 'from sklearn.metrics import f1_score')\n",
    "    test_pred = model.predict(data['x_test'])\n",
    "    print(np.round(f1_score([np.argmax(e) for e in data['y_test']], \n",
    "                            [np.argmax(e) for e in test_pred], \n",
    "                            average=None), 2))\n",
    "    model.save(DATA_FOLDER + 'keras_model_movie_reviews')\n",
    "\n",
    "    print('Model has been saved to data/.')\n",
    "    \n",
    "    # End of task\n",
    "    return model\n",
    "\n",
    "# Task: Write a function that predicts the rating of a movie review\n",
    "def predict_movie_rating(model, text) -> Any:\n",
    "    # TODO create vector representation of the input text usind the spaCy model 'de_core' from further above\n",
    "    vec = de_core(text).vector\n",
    "    # TODO make the prediction\n",
    "    prediction = model.predict(np.array([np.array(vec)]))\n",
    "\n",
    "    return np.argmax(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:58:02.809376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-07 14:58:02.809481: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-07 14:58:02.809501: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (E5790-ANAUG-W10): /proc/driver/nvidia/version does not exist\n",
      "2022-11-07 14:58:02.809768: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3039/3039 [==============================] - 21s 7ms/step - loss: 1.4998\n",
      "Epoch 2/10\n",
      "3039/3039 [==============================] - 21s 7ms/step - loss: 1.4047\n",
      "Epoch 3/10\n",
      "3039/3039 [==============================] - 21s 7ms/step - loss: 1.3687\n",
      "Epoch 4/10\n",
      "3039/3039 [==============================] - 22s 7ms/step - loss: 1.3409\n",
      "Epoch 5/10\n",
      "3039/3039 [==============================] - 22s 7ms/step - loss: 1.3092\n",
      "Epoch 6/10\n",
      "3039/3039 [==============================] - 21s 7ms/step - loss: 1.2855\n",
      "Epoch 7/10\n",
      "3039/3039 [==============================] - 29s 10ms/step - loss: 1.2615\n",
      "Epoch 8/10\n",
      "3039/3039 [==============================] - 30s 10ms/step - loss: 1.2393\n",
      "Epoch 9/10\n",
      "3039/3039 [==============================] - 21s 7ms/step - loss: 1.2188\n",
      "Epoch 10/10\n",
      "3039/3039 [==============================] - 20s 7ms/step - loss: 1.1807\n",
      "332/332 [==============================] - 1s 3ms/step\n",
      "[0.35 0.23 0.34 0.44 0.38 0.6 ]\n",
      "INFO:tensorflow:Assets written to: ./data/keras_model_movie_reviews/assets\n",
      "Model has been saved to data/.\n"
     ]
    }
   ],
   "source": [
    "# list of movie reviews and their ratings for testing purposes\n",
    "list_text_stars = [\n",
    "    [\"\"\"Ein Wunderbarer schön geschriebener Film. John Fords zeitloser Klassiker! Auf gleicher Wellenlänge mit dem Buch.\"\"\", 5],\n",
    "    [\"\"\"einfach nur hammer. und superlustig. Bin Fan der Reihe und wurde nich enttäuscht. Ganz im Gegenteil.\"\"\", 4],\n",
    "    [\"\"\"Realitätsfern, aber nett anzusehen.\"\"\", 3],\n",
    "    [\"\"\"Habe den Film heute gesehen, war ziemlich langweilig! Kaum Spannung, absolut vorhersehbar...\"\"\",0]\n",
    "]\n",
    "\n",
    "model_t1_o_10 = train_keras_model(keras_task_1_split, sampling_method=\"mediansampling\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Movie review: 'Ein Wunderbarer schön geschriebener Film. John Fords zeitloser Klassiker! Auf gleicher Wellenlänge mit dem Buch.', Prediction: 5, correct rating: 5\n",
      "-----------\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Movie review: 'einfach nur hammer. und superlustig. Bin Fan der Reihe und wurde nich enttäuscht. Ganz im Gegenteil.', Prediction: 2, correct rating: 4\n",
      "-----------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Movie review: 'Realitätsfern, aber nett anzusehen.', Prediction: 3, correct rating: 3\n",
      "-----------\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Movie review: 'Habe den Film heute gesehen, war ziemlich langweilig! Kaum Spannung, absolut vorhersehbar...', Prediction: 0, correct rating: 0\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for movie_review, rating in list_text_stars:\n",
    "    print(f\"Movie review: '{movie_review}', Prediction: {predict_movie_rating(model_t1_o_10, movie_review)}, correct rating: {rating}\")\n",
    "    print('-----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cbfed9bfced73913b8dc5c876b56c774b4b97798d853c99fd666a95989a8c72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
